# IVF Index (Inverted File Index) - Theory

## What is IVF?

**IVF = Inverted File Index**

The core idea: **Divide and conquer using clustering**

Instead of searching through ALL vectors, we:
1. **Group similar vectors into clusters** (using k-means)
2. **At search time, only check nearby clusters**
3. **Result: 10-20x speedup!**

---

## Visual Explanation

### Without IVF (Brute Force):
```
Database with 1000 vectors:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚
â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚
â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚
â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚
â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query: â­
â†’ Check ALL 1000 vectors
â†’ Time: 10ms
```

### With IVF (10 clusters):
```
Database clustered into 10 groups:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢â€¢â€¢  â”‚      â”‚      â”‚  â€¢â€¢â€¢ â”‚      â”‚
â”‚ â€¢â€¢â€¢  â”‚      â”‚      â”‚  â€¢â€¢â€¢ â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚      â”‚  â€¢â€¢â€¢ â”‚      â”‚      â”‚  â€¢â€¢â€¢ â”‚
â”‚      â”‚  â€¢â€¢â€¢ â”‚      â”‚      â”‚  â€¢â€¢â€¢ â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
  Câ‚     Câ‚‚     Câ‚ƒ     Câ‚„     Câ‚…

Query: â­ (near Câ‚)
â†’ Find nearest cluster: Câ‚
â†’ Only check 100 vectors in Câ‚
â†’ Time: 1ms (10x faster!)
```

---

## The Algorithm

### Phase 1: Building the Index (One-time)

```
1. Choose number of clusters (k)
   Example: k = 100 for 100,000 vectors

2. Run k-means clustering
   - Initialize k random centroids
   - Assign each vector to nearest centroid
   - Update centroids as mean of assigned vectors
   - Repeat until convergence

3. Store the result
   - Centroids: [Câ‚, Câ‚‚, ..., Câ‚–]
   - Inverted lists: {
       Câ‚: [vâ‚, vâ‚…, vâ‚‡, ...],
       Câ‚‚: [vâ‚‚, vâ‚ƒ, vâ‚‰, ...],
       ...
     }
```

### Phase 2: Searching (Fast!)

```
1. Find nearest centroids to query
   Example: Find 3 nearest centroids
   
2. Search only those clusters
   - Get all vectors from those 3 clusters
   - Calculate distances to query
   - Return top-k results

3. Trade-off parameter: n_probe
   - n_probe = 1: Search 1 cluster (fastest, less accurate)
   - n_probe = 5: Search 5 clusters (slower, more accurate)
   - n_probe = k: Search all clusters (same as brute force)
```

---

## K-Means Clustering - Deep Dive

### The Algorithm

```python
# Pseudocode
def kmeans(vectors, k, max_iterations=100):
    # 1. Initialize centroids randomly
    centroids = random_sample(vectors, k)
    
    for iteration in range(max_iterations):
        # 2. Assign each vector to nearest centroid
        assignments = []
        for vector in vectors:
            nearest = argmin([distance(vector, c) for c in centroids])
            assignments.append(nearest)
        
        # 3. Update centroids as mean of assigned vectors
        new_centroids = []
        for cluster_id in range(k):
            cluster_vectors = vectors[assignments == cluster_id]
            new_centroids.append(mean(cluster_vectors))
        
        # 4. Check convergence
        if centroids == new_centroids:
            break
        centroids = new_centroids
    
    return centroids, assignments
```

### Visual Example (2D)

**Iteration 0 (Random initialization):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢  â€¢  â€¢     â”‚
â”‚  â€¢ â€¢  â€¢     â”‚  Câ‚ = random
â”‚   Câ‚        â”‚  Câ‚‚ = random
â”‚             â”‚
â”‚        Câ‚‚   â”‚
â”‚      â€¢  â€¢   â”‚
â”‚     â€¢  â€¢  â€¢ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Iteration 1 (Assign to nearest):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  1  1     â”‚  Assign each point
â”‚  1 1  1     â”‚  to nearest centroid
â”‚   Câ‚        â”‚
â”‚             â”‚
â”‚        Câ‚‚   â”‚
â”‚      2  2   â”‚
â”‚     2  2  2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Iteration 2 (Update centroids):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  1  1     â”‚  Move centroids to
â”‚  1 1  1     â”‚  center of their points
â”‚    Câ‚'      â”‚  Câ‚' = mean of all 1's
â”‚             â”‚  Câ‚‚' = mean of all 2's
â”‚       Câ‚‚'   â”‚
â”‚      2  2   â”‚
â”‚     2  2  2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Iteration 3 (Converged!):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  1  1     â”‚  Centroids don't move
â”‚  1 1  1     â”‚  â†’ Converged!
â”‚   Câ‚        â”‚
â”‚             â”‚
â”‚      Câ‚‚     â”‚
â”‚      2  2   â”‚
â”‚     2  2  2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Mathematical Details

### 1. Distance to Centroid

```
For vector v and centroid c:
d(v, c) = ||v - c||

Example:
v = [1, 2, 3]
c = [4, 5, 6]
d = âˆš((1-4)Â² + (2-5)Â² + (3-6)Â²)
  = âˆš(9 + 9 + 9)
  = âˆš27 â‰ˆ 5.196
```

### 2. Centroid Update

```
New centroid = mean of all assigned vectors

C_new = (1/n) Ã— Î£(v_i) for all v_i in cluster

Example:
Cluster has 3 vectors:
vâ‚ = [1, 2, 3]
vâ‚‚ = [2, 3, 4]
vâ‚ƒ = [3, 4, 5]

C = (vâ‚ + vâ‚‚ + vâ‚ƒ) / 3
  = ([1,2,3] + [2,3,4] + [3,4,5]) / 3
  = [6, 9, 12] / 3
  = [2, 3, 4]
```

### 3. Convergence Criterion

```
Stop when centroids don't change:
||C_new - C_old|| < threshold

Or after max iterations (e.g., 100)
```

---

## Time Complexity Analysis

### Building the Index

```
K-means:
- Iterations: I (typically 10-100)
- Per iteration:
  - Assign: O(n Ã— k Ã— d) where n=vectors, k=clusters, d=dimensions
  - Update: O(n Ã— d)
- Total: O(I Ã— n Ã— k Ã— d)

Example:
n = 100,000 vectors
k = 100 clusters
d = 128 dimensions
I = 20 iterations
â†’ ~2.5 billion operations (takes a few seconds)
```

### Searching

```
Without IVF:
- Check all vectors: O(n Ã— d)
- Example: 100,000 Ã— 128 = 12.8M operations

With IVF (n_probe clusters):
- Find nearest centroids: O(k Ã— d)
- Search n_probe clusters: O((n/k) Ã— n_probe Ã— d)
- Total: O(k Ã— d + (n/k) Ã— n_probe Ã— d)

Example (n_probe = 5):
- Find centroids: 100 Ã— 128 = 12.8K operations
- Search clusters: (100,000/100) Ã— 5 Ã— 128 = 640K operations
- Total: ~650K operations (20x faster!)
```

---

## Space Complexity

```
Storage needed:
1. Centroids: k Ã— d floats
2. Vectors: n Ã— d floats (same as before)
3. Cluster assignments: n integers
4. Inverted lists: n pointers

Example:
n = 100,000 vectors
k = 100 clusters
d = 128 dimensions

Centroids: 100 Ã— 128 Ã— 4 bytes = 51 KB
Vectors: 100,000 Ã— 128 Ã— 4 bytes = 51 MB
Assignments: 100,000 Ã— 4 bytes = 400 KB
Total: ~52 MB (minimal overhead!)
```

---

## Trade-offs

### Choosing k (number of clusters)

```
Too few clusters (k = 10):
âœ… Fast to build
âœ… Fast centroid search
âŒ Large clusters â†’ slow search
âŒ Less accurate

Too many clusters (k = 10,000):
âŒ Slow to build
âŒ Slow centroid search
âœ… Small clusters â†’ fast search
âœ… More accurate

Sweet spot: k = âˆšn
- 1,000 vectors â†’ k = 32
- 10,000 vectors â†’ k = 100
- 100,000 vectors â†’ k = 316
- 1,000,000 vectors â†’ k = 1,000
```

### Choosing n_probe (clusters to search)

```
n_probe = 1:
âœ… Fastest (only 1 cluster)
âŒ Lowest accuracy (~70%)

n_probe = 5:
âœ… Good balance
âœ… Good accuracy (~90%)
âš–ï¸ Medium speed

n_probe = 20:
âš–ï¸ Slower
âœ… High accuracy (~98%)

n_probe = k:
âŒ Same as brute force
âœ… 100% accuracy
```

---

## Accuracy vs Speed

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     â”‚
â”‚  100% â”¤                          â€¢  â”‚ Brute Force
â”‚       â”‚                       â€¢     â”‚
â”‚       â”‚                    â€¢        â”‚
â”‚  95%  â”¤                 â€¢           â”‚ IVF (n_probe=10)
â”‚       â”‚              â€¢              â”‚
â”‚       â”‚           â€¢                 â”‚
â”‚  90%  â”¤        â€¢                    â”‚ IVF (n_probe=5)
â”‚       â”‚     â€¢                       â”‚
â”‚       â”‚  â€¢                          â”‚
â”‚  85%  â”¤â€¢                            â”‚ IVF (n_probe=1)
â”‚       â”‚                             â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         1x   5x   10x  20x  50x     â”‚
â”‚              Speedup                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Real-World Example

### Scenario: Document Search

```
Database: 1 million documents (1536D embeddings)
Query: "How to use Python for AI?"

Without IVF:
- Check 1,000,000 vectors
- Time: ~1 second per query
- Throughput: 1 QPS (query per second)

With IVF (k=1000, n_probe=5):
- Check 1,000 centroids: 0.1ms
- Search 5 clusters: ~5,000 vectors
- Time: ~5ms per query
- Throughput: 200 QPS
- Speedup: 200x faster!
- Accuracy: ~95% (good enough!)
```

---

## Summary

### IVF Index

**Algorithm:**
1. Cluster vectors using k-means
2. At search time, find nearest clusters
3. Only search those clusters

**Complexity:**
- Build: O(I Ã— n Ã— k Ã— d)
- Search: O(k Ã— d + (n/k) Ã— n_probe Ã— d)
- Space: O(n Ã— d + k Ã— d)

**Parameters:**
- `k`: Number of clusters (typically âˆšn)
- `n_probe`: Clusters to search (1-20)

**Performance:**
- Speed: 10-50x faster than brute force
- Accuracy: 85-95% depending on n_probe
- Memory: Minimal overhead

**Best for:**
- Medium to large datasets (10K - 10M vectors)
- When 90-95% accuracy is acceptable
- Production systems with good balance

---

## Next: Implementation!

Now that you understand the theory, let's implement IVF from scratch! ğŸš€

